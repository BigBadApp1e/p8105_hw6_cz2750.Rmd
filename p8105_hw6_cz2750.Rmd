---
title: "p8105_hw6_cz2750"
author: "Congrui Zhang"
date: "2023-12-02"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(knitr)
library(janitor)
library(readxl)
library(broom)
library(boot)


knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Q1
## Data import and wranging 
```{r message=F}
homicide <- read_csv("homicide-data.csv")

hom_data <- homicide %>%
  mutate(city_state = paste(city, state, sep = ", "),
    is_solved = if_else(disposition == "Closed by arrest", 1, 0),
    victim_age = as.numeric(victim_age)) %>%

  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")),
    victim_race %in% c("White", "Black"))
```

## Logistic regression analysis for Baltimore
```{r}
# Filter and prepare data for Baltimore, MD
baltimore_data <- filter(hom_data, city_state == "Baltimore, MD")
# Prepare data
baltimore_data <- baltimore_data %>%
  mutate(victim_sex = as.factor(victim_sex),
         victim_race = as.factor(victim_race),
         is_solved = factor(is_solved, levels = c(0, 1)))
# Fit logistic regression model
model_baltimore <- glm(is_solved ~ victim_age + victim_sex + victim_race, 
                       data = baltimore_data, family = "binomial")
# Apply broom::tidy to the model object
tidy <- tidy(model_baltimore)
# Displaying the tidy model summary
print(tidy)
#  Calculate adjusted odds ratios 
adjusted_or <- exp(coef(model_baltimore)["victim_sexMale"])  
CI <- confint.default(model_baltimore) 
CI_adjusted_or <- exp(CI["victim_sexMale", ])
# Displaying the adjusted odds ratio and its confidence interval
list(adjusted_odds_ratio = adjusted_or, confidence_interval = CI_adjusted_or)
```
### Interpretation
The adjusted odds ratio for 'victim_sexMale' is approximately 0.426. This suggests that the odds of a homicide being solved when the victim is male are about 42.6% of the odds when the victim is female, given that all other variables in the model are held constant.

The 95% confidence interval ranges from approximately 0.324 to 0.558. The fact that this interval does not include 1 is indicative of statistical significance. It implies that you can be 95% confident that the true adjusted odds ratio, in the population from which your sample is drawn, lies within this range.

## GLM analysis for each city
```{r}
# Group by city and nest data
nested_data <- hom_data %>%
  group_by(city_state) %>%
  nest()
# Fit logistic regression model and tidy with confidence intervals for each city
nested_data <- nested_data %>%
  mutate(model = map(data, ~glm(is_solved ~ victim_age + victim_sex + victim_race, 
                                data = .x, family = binomial())),
         tidied = map(model, ~tidy(.x, conf.int = TRUE)))
# Extract coefficients for `victim_sexMale`
nested_data <- nested_data %>%
  mutate(ORs = map(tidied, ~filter(.x, term == "victim_sexMale") %>%
                     mutate(OR = exp(estimate),
                            CI_lower = exp(conf.low),
                            CI_upper = exp(conf.high))))
# Unnest and organize results
GLM_results <- nested_data %>%
  select(city_state, ORs) %>%
  unnest(ORs) %>%
  select(city_state, OR, CI_lower, CI_upper)
```

## Creating the Plot
```{r}
GLM_results <- GLM_results %>%
  arrange(OR)

GLM_plot <- ggplot(GLM_results, aes(x = reorder(city_state, OR), y = OR)) +
  geom_point() +  
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper), width = 0.2) +  
  coord_flip() +
  labs(x = "Cities",
       y = "Adjusted ORs with CIs") +
  theme_minimal()
# Displaying the plot
GLM_plot
```

### Interpretation

1. Trend of ORs Less Than 1: The majority of cities have odds ratios (ORs) less than 1. This could be reflective of a systemic issue or pattern across these cities where homicides with male victims have a lower likelihood of being resolved compared to those with female victims.

2. Statistical Significance: The fact that many confidence intervals (CIs) cross the OR of 1 suggests that, for a number of cities, the observed differences might not be statistically significant. This could mean that the apparent trend of lower ORs for male victims may not hold strong statistical support in every city.

3. Variability in CIs: The width of the CIs could indicate variability in the sample sizes or data quality across different cities. Wide CIs might imply that the data from those cities is more variable or less certain, while narrow CIs suggest more confidence in the precision of the estimate.


# Q2
## Prepare the data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

## Fit the initial regression model
```{r}
initial_model <- lm(tmax ~ tmin + prcp, data = weather_df)

# Function to perform a single bootstrap step
bootstrap_step <- function(data) {
  sample_data <- data %>% sample_n(nrow(data), replace = TRUE)
  model <- lm(tmax ~ tmin + prcp, data = sample_data)
  
  # Extracting R-squared
  r_squared <- glance(model)$r.squared
  
  # Extracting coefficients
  coeffs <- tidy(model)
  beta1 <- coeffs$estimate[coeffs$term == "tmin"]
  beta2 <- coeffs$estimate[coeffs$term == "prcp"]
  
  # Conditional check to prevent log of non-positive numbers
  if(beta1 * beta2 <= 0) {
    log_product <- NA  # Assign NA if product is not positive
  } else {
    log_product <- log(beta1 * beta2)
  }
  
  return(data.frame(r_squared = r_squared, log_product = log_product))
}
```

## Perform bootstrap sampling
```{r}
set.seed(123)  # For reproducibility
bootstrap_samples <- replicate(5000, bootstrap_step(weather_df), simplify = FALSE) %>% bind_rows()
```

## Plot distributions
```{r}
ggplot(bootstrap_samples, aes(x = r_squared)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  labs(title = "Distribution of R-squared Estimates", x = "R-squared", y = "Frequency")

ggplot(bootstrap_samples, aes(x = log_product)) +
  geom_histogram(bins = 30, fill = "red", alpha = 0.7) +
  labs(title = "Distribution of Log Product Estimates", x = "Log Product", y = "Frequency")
```

## Calculate Confidence Intervals
```{r}
r_squared_ci <- quantile(bootstrap_samples$r_squared, c(0.025, 0.975), na.rm = TRUE)
log_product_ci <- quantile(bootstrap_samples$log_product, c(0.025, 0.975), na.rm = TRUE)
```

## Displaying the confidence intervals
```{r}
r_squared_ci
log_product_ci
```
### Interpretation of the Results:
95% Confidence Interval of $\hat{r}^2$ :(0.8882079 to 0.9402552)

95% Confidence Interval of $log(\beta_{1} * \beta_{2})$: (-9.063214 to -4.619267)

# Q3
## Read data
```{r}
data <- read.csv("birthweight.csv")
```

## Data Cleaning
```{r}
data <- data %>%
  mutate(
    babysex = factor(babysex, levels = c(1, 2), labels = c("Male", "Female")),
    frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9), labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    malform = factor(malform, levels = c(0, 1), labels = c("Absent", "Present")),
    mrace = factor(mrace, levels = c(1, 2, 3, 4, 8), labels = c("White", "Black", "Asian", "Puerto Rican", "Other"))
  )

# Check for missing values
summary(data)
```

There is no missing data. The 20 variables include: babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malform, menarche, mheight, momage, mrace, parity, pnumlbw, pnumsga, ppbmi, ppwt, smoken, wtgain.

## Regression Model Proposal
```{r}
model_birthweight <- lm(bwt ~ mrace + frace + momage + gaweeks, data = data)

summary(model_birthweight) %>% 
  tidy() %>%
  select(term, estimate, p.value)
summary(model_birthweight) %>% 
  glance()
```
Model Fit: The model accounts for about 22.17% of the variance in birth weight. This suggests that while the included factors do have some relationship with birth weight, there are still many other unaccounted factors.

Significant Predictors: The gestational age is a highly significant predictor, indicating that as gestational age increases, the birth weight also increases significantly (about 59.80 grams per week). Additionally, being born to Black mothers is associated with a significant decrease in birth weight compared to White mothers.

Non-significant Predictors: The mother's age and the father's race, along with other races of the mother, did not show a statistically significant impact on birth weight in this model.

Model Adequacy: The F-statistic is significant, which indicates that the model is statistically significant overall. However, the relatively low R-squared value implies that other factors not included in the model might influence birth weight.

## Residuals Plot
```{r}
data_with_preds <- data %>%
  add_predictions(model_birthweight) %>%
  add_residuals(model_birthweight)

# Plot the residuals against fitted values
ggplot(data_with_preds, aes(x = pred, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuals vs. Fitted Plot", x = "Fitted values", y = "Residuals")
```

The "Residuals vs. Fit Plot" for the birth weight model shows that the residuals are mostly randomly dispersed around the zero point of the horizontal line, which is consistent with the assumptions of homoscedasticity and linearity. There was no clear pattern indicating the presence of nonlinearity or heteroscedasticity.

## Model Comparison
```{r}
# Define the models for comparison
set.seed(77)

cv_dataset <-
  data %>% 
  crossv_mc(n = 100,test = 0.2)
  

cv_df <- 
  cv_dataset %>%
   mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df <-
  cv_df %>%
    mutate(
    my_model  = map(train, ~lm(bwt ~ mrace + frace + momage + gaweeks, data = .x)),
    model_frace_gaweeks = map(train, ~lm(bwt ~ frace + gaweeks, data = .x)),
    model_interactions  = map(train, ~lm(bwt ~ (mrace + momage + gaweeks)^3, data = .x))
    ) %>%
   mutate(
    rmse_my_model = map2_dbl(my_model, test, ~rmse(model = .x, data = .y)),
    rmse_frace_gaweeks = map2_dbl(model_frace_gaweeks, test, ~rmse(model = .x, data = .y)),
    rmse_interactions = map2_dbl(model_interactions, test, ~rmse(model = .x, data = .y))
   )
```

## Reporting
```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_boxplot() +
  labs(title = 
  "Prediction Error Distributions across Models", 
       x = "Models", y = "Root Mean Square Error")  +
  scale_x_discrete(
    labels = c("My Model", "Father’s race + Gestational age in weeks ", "Interactions Model")) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))



```

What we are comparing here is the cross-validation prediction error of the models. Overall, my model seems to have the lowest prediction error (rmse) and therefore is probably the best model .
